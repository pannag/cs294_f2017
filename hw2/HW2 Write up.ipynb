{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete CartPole-v0\n",
    "```\n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 5 -dna --exp_name\n",
    "sb_no_rtg_dna \n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 5 -rtg -dna --exp_name\n",
    "sb_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 1000 -e 5 -rtg --exp_name\n",
    "sb_rtg_na\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 5 -dna --exp_name\n",
    "lb_no_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 5 -rtg -dna --exp_name\n",
    "lb_rtg_dna\n",
    "python train_pg.py CartPole-v0 -n 100 -b 5000 -e 5 -rtg --exp_name\n",
    "lb_rtg_na\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small batch experiments\n",
    "<img src=\"Figure_Cartpole-v0.png\">\n",
    "\n",
    "### Large batch experiments\n",
    "<img src=\"Figure_lb_Cartpole-v0.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Clearly, *reward_to_go=True* (\"rtg\") gives better results than the trajectory-centric one (\"no_rtg\"), \n",
    "- adding  advantage-centering i.e. *normalize_advantages=True* (\"rtg_na\") seems to only help marginally. \n",
    "- Using a large batch size helps the algorithm to converge faster, especially in the case of trajectory-centric PG estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InvertedPendulum-v1 continuous control\n",
    "Ran with the Roboschool environment.\n",
    "\n",
    "### Small batch\n",
    "```\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 1000 -e 5 -rtg --exp_name sb_rtg_na\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 1000 -e 5 -rtg -dna --exp_name sb_rtg_dna\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 1000 -e 5 --exp_name sb_no_rtg_na\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 1000 -e 5 -dna --exp_name sb_no_rtg_dna\n",
    "```\n",
    "<img src=\"Figure_RoboInvertedPendulum-v1.png\">\n",
    "\n",
    "### Large batch\n",
    "```\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 5000 -e 5 -rtg --exp_name lb_rtg_na\n",
    "python train_pg.py RoboschoolInvertedPendulum-v1 -n 100 -b 5000 -e 5 -rtg -dna --exp_name lb_rtg_dna\n",
    "```\n",
    "<img src=\"Figure_lb_RoboInvertedPendulum-v1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- *reward_to_go=True* (\"rtg\") gives better results than the trajectory-centric one (\"no_rtg\"), but only if   advantage-centering is not used i.e. *normalize_advantages=False* (\"rtg_dna\"). \n",
    "- The rewards go down after certain iterations. Seems like we are overfitting past 60 iterations.\n",
    "- The large batch helps the convergence rate and stability.\n",
    "\n",
    "This was using default parameters. Might be worth experimenting with learning rate and batch sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Baseline\n",
    "Policy gradient with state estimated baseline. \n",
    "\n",
    "Fitted a TD(0) based baseline, i.e. `target_baseline[i] = reward[i] + gamma * prev_baseline[i+1]`.\n",
    "See for more context the [last equation on the slide 21 of the fall'17 lecture](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf).\n",
    "\n",
    "Here's a plot comparing the outcomes for Inverted Pendulum with and without baseline. \n",
    "<img src=\"Figure_RoboInvertedPendulum-v1-baseline1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- The baseline does not seem to help, at least in the current parameter setting. \n",
    "- That said, the rewards go down after certain iterations. Seems like past 80 iterations, the baseline performs better, although in absolute terms, both are bad.\n",
    "\n",
    "This was using default parameters. Might be worth experimenting with learning rate and batch sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half Cheetah\n",
    "Ran with the Roboschool environment.\n",
    "```\n",
    "python train_pg.py RoboschoolHalfCheetah-v1 -ep 150 --discount 0.9 -n 100 -b 1000 -e 5 -rtg --exp_name sb_rtg_na\n",
    "python train_pg.py RoboschoolHalfCheetah-v1 -ep 150 --discount 0.9 -n 100 -b 1000 -e 5 -rtg -bl --exp_name sb_rtg_na_bl\n",
    "python train_pg.py RoboschoolHalfCheetah-v1 -ep 150 --discount 0.9 -n 100 -b 1000 -e 5 -rtg --n_layers 2 -bl --exp_name sb_rtg_na_bl_2L\n",
    "python train_pg.py RoboschoolHalfCheetah-v1 -ep 150 --discount 0.9 -n 100 -b 5000 -e 5 -rtg --n_layers 2 -bl --exp_name lb_rtg_na_bl_2L\n",
    "python train_pg.py RoboschoolHalfCheetah-v1 -ep 150 --discount 0.9 -n 100 -b 1000 -e 5 -rtg -dna --exp_name sb_rtg_dna\n",
    "```\n",
    "<img src=\"Figure_RoboHalfCheetah-v1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- The rtg, baseline and baseline with 2 layers all give the same performance (Avg return ~ 20). \n",
    "- Increasing the batch size to 5000 helps marginally (Avg return ~30)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
